"""
æ£€æŸ¥åˆå¹¶åçš„æ•°æ®é›†ä¸­ train å’Œ val ç›®å½•æ˜¯å¦æœ‰é‡å¤æ–‡ä»¶
"""
from pathlib import Path
from collections import defaultdict

def check_duplicates(dataset_dir):
    """æ£€æŸ¥ train å’Œ val ç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶"""
    train_images = dataset_dir / "train" / "images"
    val_images = dataset_dir / "valid" / "images"

    # æå–æ‰€æœ‰åŸºç¡€æ–‡ä»¶å
    train_files = set()
    val_files = set()

    # è¯»å– train ç›®å½•
    if train_images.exists():
        for img_path in train_images.glob("*.jpg"):
            # æå–åŸºç¡€æ–‡ä»¶åï¼ˆå»æ‰æ•°æ®é›†å‰ç¼€ï¼‰
            # æ ¼å¼: {dataset_name}_{original_name}
            parts = img_path.name.split('_', 1)
            if len(parts) == 2:
                base_name = parts[1]
                train_files.add(base_name)

    # è¯»å– val ç›®å½•
    if val_images.exists():
        for img_path in val_images.glob("*.jpg"):
            parts = img_path.name.split('_', 1)
            if len(parts) == 2:
                base_name = parts[1]
                val_files.add(base_name)

    # æ‰¾å‡ºäº¤é›†
    duplicates = train_files & val_files

    return duplicates, len(train_files), len(val_files)

def main():
    dataset_dir = Path("merged_basketball_dataset")

    print("=" * 70)
    print("æ£€æŸ¥é‡å¤æ–‡ä»¶")
    print("=" * 70)

    duplicates, train_count, val_count = check_duplicates(dataset_dir)

    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   - train ç›®å½•æ–‡ä»¶æ•°: {train_count}")
    print(f"   - val ç›®å½•æ–‡ä»¶æ•°: {val_count}")
    print(f"   - é‡å¤æ–‡ä»¶æ•°: {len(duplicates)}")

    if duplicates:
        print(f"\nâš ï¸  å‘ç°é‡å¤æ–‡ä»¶:")
        for i, dup in enumerate(list(duplicates)[:10]):
            print(f"   {i+1}. {dup}")
        if len(duplicates) > 10:
            print(f"   ... è¿˜æœ‰ {len(duplicates) - 10} ä¸ªé‡å¤æ–‡ä»¶")
    else:
        print("\nâœ… æœªå‘ç°é‡å¤æ–‡ä»¶ï¼")

    print(f"\nğŸ“ æ£€æŸ¥ç›®å½•:")
    print(f"   {dataset_dir}")

if __name__ == "__main__":
    main()
